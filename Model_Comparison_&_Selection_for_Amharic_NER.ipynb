{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFWjHLRtyIt1"
      },
      "outputs": [],
      "source": [
        "# Notebook 4: Model Comparison & Selection for Amharic NER\n",
        "# ========================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets seqeval torch accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "\n",
        "# ==============================\n",
        "# 1. Define Models to Compare\n",
        "# ==============================\n",
        "model_names = [\n",
        "    \"xlm-roberta-base\",  # Large multilingual model\n",
        "    \"bert-base-multilingual-cased\",  # Multilingual BERT\n",
        "    \"afroxmlr/bert-tiny-amharic\"  # Small Amharic-specific model\n",
        "]\n",
        "\n",
        "label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
        "\n",
        "# ==============================\n",
        "# 2. Load Preprocessed Dataset\n",
        "# ==============================\n",
        "# Load your tokenized dataset from Notebook 3\n",
        "dataset = Dataset.from_file(\"data/labeled/ner_train.conll\")  # Adjust if necessary\n",
        "\n",
        "# Function to tokenize and align labels (reuse from Notebook 3)\n",
        "def tokenize_and_align_labels(examples, tokenizer):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label_list.index(label[word_idx]))\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# ==============================\n",
        "# 3. Compare Models\n",
        "# ==============================\n",
        "results_summary = []\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"\\nTraining and evaluating model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenized_dataset = dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_name.replace('/', '_')}\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"./logs/{model_name.replace('/', '_')}\",\n",
        "        logging_steps=10\n",
        "    )\n",
        "\n",
        "    def compute_metrics(p):\n",
        "        predictions, labels = p\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "        true_labels, true_predictions = [], []\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            true_labels.append([label_list[l] for l in label if l != -100])\n",
        "            true_predictions.append([label_list[p] for (p, l) in zip(predictions[i], label) if l != -100])\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "            \"precision\": precision_score(true_labels, true_predictions),\n",
        "            \"recall\": recall_score(true_labels, true_predictions),\n",
        "            \"f1\": f1_score(true_labels, true_predictions)\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        eval_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    results_summary.append({\"model\": model_name, **metrics})\n",
        "\n",
        "# ==============================\n",
        "# 4. Compare Results\n",
        "# ==============================\n",
        "df_results = pd.DataFrame(results_summary)\n",
        "print(\"\\nModel Comparison Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# Optionally save results\n",
        "df_results.to_csv(\"data/ner_model_comparison.csv\", index=False)\n"
      ]
    }
  ]
}