{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThrxcZ6vxvuG"
      },
      "outputs": [],
      "source": [
        "# Notebook 3: Fine-Tuning NER Model (Amharic)\n",
        "# ============================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets seqeval torch accelerate\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# ==============================\n",
        "# 1. Load Labeled CoNLL Data\n",
        "# ==============================\n",
        "def read_conll(file_path):\n",
        "    \"\"\"\n",
        "    Read a CoNLL formatted file into a list of dicts\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "\n",
        "    tokens, labels = [], []\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        if line.strip() == \"\":\n",
        "            if tokens:\n",
        "                data.append({\"tokens\": tokens, \"labels\": labels})\n",
        "                tokens, labels = [], []\n",
        "        else:\n",
        "            splits = line.split()\n",
        "            tokens.append(splits[0])\n",
        "            labels.append(splits[1])\n",
        "    return data\n",
        "\n",
        "train_data = read_conll(\"data/labeled/ner_train.conll\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(train_data)\n",
        "\n",
        "# ==============================\n",
        "# 2. Load Tokenizer & Model\n",
        "# ==============================\n",
        "model_name = \"xlm-roberta-base\"  # multilingual model, works with Amharic\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label_list.index(label[word_idx]))\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# ==============================\n",
        "# 3. Training Setup\n",
        "# ==============================\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
        "data_collator = DataCollatorForTokenClassificati_\n"
      ]
    }
  ]
}